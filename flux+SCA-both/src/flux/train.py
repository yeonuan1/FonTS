<<<<<<< HEAD
# train.py
=======
# -*- coding: utf-8 -*-
>>>>>>> theirs/main

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
<<<<<<< HEAD
from transformers import (
    T5Tokenizer, T5EncoderModel, AutoProcessor, SiglipTextModel,
    get_cosine_schedule_with_warmup
)
=======
from transformers import AutoTokenizer # ETC-Token ì²˜ë¦¬ë¥¼ ìœ„í•´ í† í¬ë‚˜ì´ì € ì‚¬ìš© ê°€ì •
>>>>>>> theirs/main
import math
import os
import numpy as np
import json
<<<<<<< HEAD
import yaml # PyYAML (pip install pyyaml)
import argparse
from PIL import Image
from einops import rearrange, repeat
from tqdm.auto import tqdm
from pathlib import Path

from accelerate import Accelerator
import wandb

# flux imports
from flux.util import load_flow_model, load_ae
from flux.modules.layers import (
    DoubleStreamBlockLoraProcessor, 
    SingleStreamBlockLoraProcessor, 
    timestep_embedding
)

# Helper Functions/Classes 

class TextProjection(nn.Module): # <-- ìˆ˜ì •ëœ ì½”ë“œ
    def __init__(self, input_dim=768, output_dim=4096, dtype=None): # 1. dtype ë°›ê¸°
        super().__init__()
        self.proj = nn.Sequential(
            # vvv 2. ë°›ì€ dtypeì„ nn.Linearì™€ LayerNormì— ì „ë‹¬ vvv
            nn.Linear(input_dim, output_dim, dtype=dtype),
            nn.LayerNorm(output_dim, dtype=dtype),
        )
    def forward(self, x):
        return self.proj(x)

class TCDataset(Dataset):
=======
from PIL import Image
import random

from .sampling import prepare
from .util import load_flow_model, load_ae

os.environ['AE'] = '/data1/FonTS/flux+SCA-both/src/models/ae.safetensors'
os.environ['FLUX_DEV'] = '/data1/FonTS/flux+SCA-both/src/models/flux1-dev.safetensors'


"""
 ì‹¤ì œ $\text{Flux}$ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜(load_ae, load_flow_model, load_t5, load_clip) ë° 
 $\text{Flux}$ í´ë˜ìŠ¤ ì •ì˜ê°€ ì™¸ë¶€ íŒŒì¼(utils.py, model.py ë“±)ì— ì •í™•íˆ ì¡´ì¬í•˜ê³  í˜¸ì¶œ ê°€ëŠ¥í•¨ì„ ê°€ì •
"""

# ====================================================================
# 1. íŒŒë¼ë¯¸í„° ë° ë”ë¯¸ í´ë˜ìŠ¤ ì •ì˜ (ì™¸ë¶€ ëª¨ë“ˆì„ ëª¨ë°©)
# ====================================================================

# íŒŒë¼ë¯¸í„° ì •ì˜ (í•˜ë“œì›¨ì–´ ë° í›ˆë ¨ íš¨ìœ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ BATCH_SIZE=1 ì„¤ì • ìœ ì§€)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_TYPE = "flux-dev"
LEARNING_RATE = 1e-5
BATCH_SIZE = 1 # ë©”ëª¨ë¦¬ ì œì•½ì„ ê³ ë ¤í•˜ì—¬ 1ë¡œ ì„¤ì •
NUM_EPOCHS = 10
# â­ ì¶”ê°€: Guidance Distillation Strength ì„¤ì •
GUIDANCE_STRENGTH = 7.0 # ì¼ë°˜ì ì¸ ê°’ (4.0 ~ 10.0 ì‚¬ì´ì—ì„œ í…ŒìŠ¤íŠ¸ í•„ìš”)

# --- T5/mT5 ë¡œë”© í•¨ìˆ˜ ì •ì˜ (ì‹¤ì œ ëª¨ë“ˆì„ ëª¨ë°©í•œ Dummy í´ë˜ìŠ¤) ---
MT5_MODEL_NAME = "google/mt5-base" # mT5 ì‚¬ìš©ì„ ìœ„í•´ ë¡œë”© ì´ë¦„ ì •ì˜
ETC_TOKEN_ID_START = 50000 

class TextProjection(nn.Module):
    """mT5 ì„ë² ë”©(768ì°¨ì›)ì„ T5-XXL í˜¸í™˜(4096ì°¨ì›)ìœ¼ë¡œ ë³€í™˜"""
    def __init__(self, input_dim=768, output_dim=4096):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.LayerNorm(output_dim),  # ì•ˆì •ì„± í–¥ìƒ
        )
    
    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, 768) - mT5 ì„ë² ë”©
        Returns:
            (batch, seq_len, 4096) - T5-XXL í˜¸í™˜ ì„ë² ë”©
        """
        return self.proj(x)

class DummyHFEmbedder:
    """ì‹¤ì œ mT5/CLIP ì¸ì½”ë” ëª¨ë“ˆì„ ëŒ€ì²´"""
    def __init__(self, version, max_length, torch_dtype):
        self.output_dim = 768  # mT5/CLIP-Baseì˜ ì¼ë°˜ì ì¸ ì„ë² ë”© ì°¨ì› ê°€ì •
        # ì‹¤ì œ T5/CLIP ëª¨ë“ˆì„ ë¡œë“œí–ˆë‹¤ê³  ê°€ì •í•˜ê³ , ì„ë² ë”©ì„ ìœ„í•œ ë”ë¯¸ ë ˆì´ì–´ë¥¼ ì •ì˜
        self.hf_module = torch.nn.Linear(768, 768) 
        # mT5ì˜ ì„ë² ë”© ë ˆì´ì–´ ì´ë¦„ì€ 'shared'ì¼ ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„œëŠ” ë”ë¯¸ë¡œ ì •ì˜
        self.hf_module.shared = torch.nn.Parameter(torch.randn(50000 + 100, 768)) 
        
    def to(self, device): return self
    def eval(self): return self
    def requires_grad_(self, requires): return self
    # __call__ì€ prepare í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë¨. (B, L, D) í…ì„œ ë°˜í™˜ ê°€ì •
    def __call__(self, text): 
        return torch.randn(len(text), 50, self.output_dim, device=DEVICE) 
    
def load_clip(device) -> DummyHFEmbedder: 
    return DummyHFEmbedder("CLIP", 77, torch.bfloat16).to(device)

def load_t5(device: str | torch.device = "cuda", max_length: int = 512) -> DummyHFEmbedder: 
    print(f"Loading mT5 model: {MT5_MODEL_NAME}")
    return DummyHFEmbedder(MT5_MODEL_NAME, max_length=max_length, torch_dtype=torch.bfloat16).to(device)

class DummyAE:
    """ì‹¤ì œ VAE ì¸ì½”ë”/ë””ì½”ë”ë¥¼ ëŒ€ì²´"""
    def encode(self, x): 
        # VAEëŠ” (B, 3, H, W) -> (B, 64, H/16, W/16) (ì˜ˆì‹œ)ë¡œ ë³€í™˜
        return torch.randn(x.shape[0], 64, 32, 32, device=DEVICE) 
    def eval(self): return self
    def requires_grad_(self, requires): return self

class DummyFlux(torch.nn.Module):
    """ì‹¤ì œ Flux(MM-DiT) ë°±ë³¸ì„ ëŒ€ì²´"""
    def __init__(self):
        super().__init__()
        # ë…¼ë¬¸ì˜ txt_in ë ˆì´ì–´ë¥¼ ëª¨ë°©
        self.txt_in = nn.Linear(768, 3072) 
    def forward(self, img, img_ids, txt, txt_ids, timesteps, y, **kwargs):
        return torch.randn(img.shape[0], 64, 32, 32, device=DEVICE) 
    # ì‹¤ì œ Flux ëª¨ë¸ì´ ê°€ì§€ê³  ìˆì–´ì•¼ í•  set_etc_token_trainableì´ ì°¸ì¡°í•˜ëŠ” ì†ì„±
    # txt_inì´ nn.Moduleì´ë¯€ë¡œ parameters()ë¥¼ í†µí•´ ì ‘ê·¼ ê°€ëŠ¥í•¨

# ====================================================================
# 2. Dataset ë° íŒŒë¼ë¯¸í„° ì„¤ì • í•¨ìˆ˜
# ====================================================================

class TCDataset(Dataset):
    """metadata.jsonl ë° ì´ë¯¸ì§€ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì‹¤ì œ ë°ì´í„°ì…‹ ë¡œë”"""
>>>>>>> theirs/main
    def __init__(self, metadata_path: str, image_base_path: str = './'):
        self.metadata = []
        with open(metadata_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.metadata.append(json.loads(line))
        self.image_base_path = image_base_path
<<<<<<< HEAD
    def __len__(self):
        return len(self.metadata)
    def __getitem__(self, idx):
        item = self.metadata[idx]
        prompt_plain = item['text_plain']
        prompt_html = item['text_html']
        image_path = os.path.join(self.image_base_path, item['id'] + '.jpg')
        try:
            image = Image.open(image_path).convert("RGB")
            image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() 
            image_tensor = (image_tensor / 127.5) - 1.0 
        except Exception as e:
            return None 
        return image_tensor, prompt_plain, prompt_html

def custom_collate_fn(batch):
    batch = list(filter(lambda x: x is not None, batch))
    if not batch: return None 
    images = [item[0] for item in batch]
    prompts_plain = [item[1] for item in batch]
    prompts_html = [item[2] for item in batch]
    images_batch = torch.stack(images)
    return images_batch, prompts_plain, prompts_html

def load_mt5_with_etc_tokens(model_name: str, etc_tokens: list, device="cpu"): # <-- Load to CPU
    print(f"Loading mT5 model: {model_name}")
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5EncoderModel.from_pretrained(model_name)
    tokenizer.add_special_tokens({'additional_special_tokens': etc_tokens})
    print(f"Added {len(etc_tokens)} new ETC tokens to mT5 tokenizer.")
    model.resize_token_embeddings(len(tokenizer))
    print(f"Resized mT5 model embeddings to {len(tokenizer)}.")
    # Load to CPU, not device
    return model.to(device, dtype=torch.bfloat16), tokenizer 

def load_siglip(model_name: str, device="cpu"): # <-- Load to CPU
    print(f"Loading SigLIP model: {model_name}")
    tokenizer = AutoProcessor.from_pretrained(model_name).tokenizer
    model = SiglipTextModel.from_pretrained(model_name)
    # Load to CPU, not device
    return model.to(device, dtype=torch.bfloat16), tokenizer 


def apply_lora_to_model(model, rank: int, hidden_size: int, device, dtype): # <-- ìˆ˜ì •: dtype ì¸ì ì¶”ê°€
    """
    (FIXED) Finds all relevant attention processors and replaces them
    with LoRA-enabled processors.
    """
    lora_attn_procs = {}
    total_replaced = 0

    # Iterate over the *existing* processors
    for name, processor in model.attn_processors.items():
        if name.startswith("double_blocks"):
            lora_attn_procs[name] = DoubleStreamBlockLoraProcessor(
                dim=hidden_size, rank=rank, dtype=dtype # <-- ìˆ˜ì •: dtype ì „ë‹¬
            ).to(device) # <-- .to(device)ëŠ” ìœ ì§€
            total_replaced += 1
        elif name.startswith("single_blocks"):
            lora_attn_procs[name] = SingleStreamBlockLoraProcessor(
                dim=hidden_size, rank=rank, dtype=dtype # <-- ìˆ˜ì •: dtype ì „ë‹¬
            ).to(device) # <-- .to(device)ëŠ” ìœ ì§€
            total_replaced += 1
        else:
            # Keep the original processor if it's not a target
            lora_attn_procs[name] = processor
            
    if total_replaced > 0:
        model.set_attn_processor(lora_attn_procs)
    
    # Log the *actual* number replaced
    print(f"âœ… Applied LoRA (rank={rank}) to {total_replaced} attention blocks.")

def set_trainable_text_parts(t5_model, text_proj, etc_tokens: list):
    """
    Correctly unfreezes the projection layer and uses a
    gradient hook to train *only* the new ETC tokens.
    """
    text_proj.requires_grad_(True)
    print("âœ… Text Projection (768â†’4096): Unfrozen")

    t5_embed_layer = t5_model.get_input_embeddings()
    num_new_tokens = len(etc_tokens)

    if num_new_tokens > 0:
        # Get the embedding weight parameter
        embedding_weight = t5_embed_layer.weight
        
        # 1. Make the *entire* parameter require gradients
        embedding_weight.requires_grad = True
        
        # 2. Create a mask to identify the *old* tokens
        # (Everything *except* the last 'num_new_tokens')
        old_token_mask = torch.ones_like(embedding_weight, dtype=torch.bool)
        old_token_mask[-num_new_tokens:] = False

        # Ensure mask is on the same device
        old_token_mask = old_token_mask.to(embedding_weight.device)

        # 3. Register a hook that will be called during backward()
        def zero_grad_hook(grad):
            mask_on_device = old_token_mask.to(grad.device)
            # Zero out gradients for all *old* tokens
            grad[mask_on_device] = 0
            
            return grad
        
        # Register the hook on the weight tensor
        embedding_weight.register_hook(zero_grad_hook)
        
        print(f"âœ… mT5: {num_new_tokens} ETC-Tokens Unfrozen (using gradient hook).")
    
    return t5_embed_layer

# 3. Trainer Class 

class Trainer:
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.cfg_model = self.config['model']
        self.cfg_data = self.config['data']
        self.cfg_train = self.config['training']
        self.cfg_log = self.config['logging']

        self.accelerator = Accelerator(
            gradient_accumulation_steps=self.cfg_train['gradient_accumulation_steps'],
            mixed_precision=self.cfg_train['mixed_precision'],
            log_with="wandb" if self.cfg_log.get('use_wandb') else None,
        )

        if self.accelerator.is_main_process and self.cfg_log.get('use_wandb', False):
            self.accelerator.init_trackers(
                project_name=self.cfg_log['wandb_project'],
                config=self.config,
                init_kwargs={"wandb": {"name": self.cfg_log['wandb_run_name']}}
            )
        
        self.device = self.accelerator.device
        
        self.setup_models()
        self.setup_data()
        self.setup_optimization()
        self.prepare_for_training()
        self.global_step = 0

    def setup_models(self):
        self.accelerator.print("Setting up models...")
        
        # --- 1. ëª¨ë¸ ë¡œë“œ ---
        # (Flux, AEëŠ” GPUë¡œ, í…ìŠ¤íŠ¸ ì¸ì½”ë”ëŠ” CPU ì˜¤í”„ë¡œë”©ì„ ìœ„í•´ CPUë¡œ)
        self.flux_model = load_flow_model(self.cfg_model['flux_model_type'], self.device).to(torch.bfloat16)
        self.ae = load_ae(self.cfg_model['flux_model_type'], self.device).to(torch.bfloat16)
        
        self.t5_model, self.t5_tokenizer = load_mt5_with_etc_tokens(
          self.cfg_model['mt5_model_name'], self.cfg_model['etc_tokens'], device="cpu"
        )
        self.siglip_model, self.siglip_tokenizer = load_siglip(
            self.cfg_model['siglip_model_name'], device="cpu"
        )
        # self.text_proj = TextProjection(input_dim=768, output_dim=4096).to("cpu", dtype=torch.bfloat16)
        self.text_proj = TextProjection(
            input_dim=768, 
            output_dim=4096, 
            dtype=torch.bfloat16 # 3. ìƒì„± ì‹œ dtype ëª…ì‹œ
        ).to("cpu")
        
        # (A) LoRAë¥¼ ë¨¼ì € ì ìš©
        if self.cfg_model['use_lora']:
            model_dtype = next(self.flux_model.parameters()).dtype
            apply_lora_to_model(
                self.flux_model, 
                rank=self.cfg_model['lora_rank'], 
                hidden_size=self.flux_model.hidden_size,
                device=self.device,
                dtype=model_dtype
            ) 

        # (B) ëª¨ë“  ê¸°ë³¸ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë™ê²°
        self.flux_model.requires_grad_(False)
        self.ae.eval().requires_grad_(False)
        self.t5_model.eval().requires_grad_(False)
        self.siglip_model.eval().requires_grad_(False)
        self.text_proj.requires_grad_(False)
        self.accelerator.print("Froze all base model weights.")

        # (C) LoRA, TextProj, ETC í† í°ë§Œ ë‹¤ì‹œ í™œì„±í™”(Unfreeze)
        # (C-1) LoRA íŒŒë¼ë¯¸í„° í™œì„±í™”
        if self.cfg_model['use_lora']:
            lora_count = 0
            for name, param in self.flux_model.named_parameters():
                if "lora" in name.lower():
                    param.requires_grad = True
                    lora_count += 1
            if lora_count == 0:
                self.accelerator.print("âš ï¸ WARNING: No LoRA parameters found to unfreeze.")
            else:
                self.accelerator.print(f"âœ… Unfroze {lora_count} LoRA parameters.")
            
        # (C-2) í…ìŠ¤íŠ¸ ê´€ë ¨ íŒŒë¼ë¯¸í„° í™œì„±í™”
        self.t5_embed_layer = set_trainable_text_parts(
            self.t5_model, self.text_proj, self.cfg_model['etc_tokens']
        )
        
        if self.cfg_train.get('gradient_checkpointing', False):
            self.flux_model.gradient_checkpointing = True
            self.accelerator.print("âœ… Gradient Checkpointing enabled for Flux model.")

        # (D) ë¹„í•™ìŠµ ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •
        self.ae.eval()
        self.siglip_model.eval()
        self.t5_model.eval()
    def setup_data(self):
        self.accelerator.print("Setting up datasets...")
        train_dataset = TCDataset(
            metadata_path=self.cfg_data['metadata_path'], 
            image_base_path=self.cfg_data['image_base_path']
        )
        self.train_dataloader = DataLoader(
            train_dataset, 
            batch_size=self.cfg_train['batch_size'], 
            shuffle=True, 
            collate_fn=custom_collate_fn,
            num_workers=self.cfg_data['num_workers']
        )

    def setup_optimization(self):
        self.accelerator.print("Setting up optimizer and scheduler...")
        
        # Correctly filter for *all* params that require grad
        lora_params = [p for p in self.flux_model.parameters() if p.requires_grad]
        proj_params = [p for p in self.text_proj.parameters() if p.requires_grad]
        t5_embed_params = [p for p in self.t5_embed_layer.parameters() if p.requires_grad]
        
        trainable_params = lora_params + proj_params + t5_embed_params

        if not trainable_params:
            self.accelerator.print("âš ï¸ WARNING: No trainable parameters found!")
        
        # Log the *correct* counts
        self.accelerator.print(f"ğŸ“Š Trainable parameters:")
        self.accelerator.print(f"  - Flux (LoRA): {sum(p.numel() for p in lora_params):,}")
        self.accelerator.print(f"  - mT5 (ETC):   {sum(p.numel() for p in t5_embed_params):,}")
        self.accelerator.print(f"  - Projection:  {sum(p.numel() for p in proj_params):,}")
        
        self.optimizer = optim.AdamW(trainable_params, lr=self.cfg_train['learning_rate'])
        
        num_training_steps = (
            len(self.train_dataloader) * self.cfg_train['num_epochs']
        ) // self.cfg_train['gradient_accumulation_steps']
        num_warmup_steps = int(num_training_steps * 0.1)
        
        self.lr_scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
        )

    def prepare_for_training(self):
        self.accelerator.print("Preparing models with accelerator...")
        
        # Prepare only the *trainable* models
        (
            self.flux_model,
            self.text_proj,
            self.t5_embed_layer,
            self.optimizer,
            self.train_dataloader,
            self.lr_scheduler
        ) = self.accelerator.prepare(
            self.flux_model,
            self.text_proj,
            self.t5_embed_layer,
            self.optimizer,
            self.train_dataloader,
            self.lr_scheduler
        )
        
        # AE stays on GPU, but is not prepared (no gradients)
        self.ae.to(self.device)
        # Text encoders stay on CPU, are not prepared
        
    def training_step(self, batch):
        x0_pixel, prompts_plain, prompts_html = batch
        
        with torch.no_grad():
            x0_latent = self.ae.encode(x0_pixel.to(self.device, dtype=torch.bfloat16))
            
        batch_size = x0_latent.shape[0]
        t = torch.rand(batch_size, device=self.device, dtype=torch.bfloat16)
        epsilon = torch.randn_like(x0_latent, device=self.device, dtype=torch.bfloat16)
        zt = x0_latent * (1 - t)[:, None, None, None] + epsilon * t[:, None, None, None]
        guidance_tensor = torch.full(
            (batch_size,), self.cfg_train['guidance_strength'], device=self.device, dtype=torch.bfloat16
        )

        # --- CPU Offloading ---
        # (C-1) SigLIP 2 (vec) - Move, encode, offload
        self.siglip_model.to(self.device)
        with torch.no_grad():
            siglip_inputs = self.siglip_tokenizer(
                prompts_plain, padding="max_length", max_length=64, # <-- Corrected length
                truncation=True, return_tensors="pt"
            ).to(self.device)
            vec_cond = self.siglip_model(**siglip_inputs).pooler_output.to(dtype=torch.bfloat16)
        self.siglip_model.cpu() # Offload back to CPU

        # (C-2) mT5 (txt) - Move, encode, offload
        self.t5_model.to(self.device)
        t5_inputs = self.t5_tokenizer(
            prompts_html, padding="max_length", max_length=512, 
            truncation=True, return_tensors="pt"
        ).to(self.device)
        
        # Get embeddings from the *prepared* (GPU) embedding layer
        t5_embeds = self.t5_embed_layer(t5_inputs.input_ids)
        
        with torch.no_grad():
            txt_cond_768 = self.t5_model(
                inputs_embeds=t5_embeds,
                attention_mask=t5_inputs.attention_mask
            ).last_hidden_state.to(dtype=torch.bfloat16)
        self.t5_model.cpu() # Offload back to CPU
        # --- End Offloading ---
        with self.accelerator.autocast():
            # (C-3) Text Projection (Trainable)
            txt_cond = self.text_proj(txt_cond_768)
            
            # (C-4) ID í…ì„œ ì¤€ë¹„
            txt_ids = torch.zeros(
                txt_cond.shape[0], txt_cond.shape[1], 3, device=self.device, dtype=torch.bfloat16
            )
            img_cond = rearrange(zt, "b c (h ph) (w pw) -> b (h w) (c ph pw)", ph=2, pw=2)
            h, w = zt.shape[2] // 2, zt.shape[3] // 2
            img_ids_template = torch.zeros(h, w, 3, device=self.device)
            img_ids_template[..., 1] = img_ids_template[..., 1] + torch.arange(h, device=self.device)[:, None]
            img_ids_template[..., 2] = img_ids_template[..., 2] + torch.arange(w, device=self.device)[None, :]
            img_ids = repeat(img_ids_template, "h w c -> b (h w) c", b=batch_size).to(torch.bfloat16)

            # (D) Flux MM-DiT ìˆœì „íŒŒ (Trainable)
            noise_pred = self.flux_model(
                img=img_cond,
                img_ids=img_ids,
                txt=txt_cond,
                txt_ids=txt_ids,
                timesteps=t,
                y=vec_cond,
                guidance=guidance_tensor,
            )

            # (h, w were defined earlier as zt.shape[2] // 2, zt.shape[3] // 2)
            noise_pred = rearrange(
                noise_pred, 
                "b (h w) (c ph pw) -> b c (h ph) (w pw)", 
                h=h, w=w, ph=2, pw=2
            )

            # (E) ì†ì‹¤ ê³„ì‚°
            loss = F.mse_loss(noise_pred.float(), epsilon.float())

        return loss

    def train(self):
        self.accelerator.print("\n" + "="*80)
        self.accelerator.print("Starting training...")
        self.accelerator.print("="*80 + "\n")

        for epoch in range(self.cfg_train['num_epochs']):
            self.flux_model.train()
            self.text_proj.train()
            self.t5_embed_layer.train()

            progress_bar = tqdm(
                self.train_dataloader,
                disable=not self.accelerator.is_local_main_process,
                desc=f"Epoch {epoch+1}/{self.cfg_train['num_epochs']}"
            )
            
            for step, batch in enumerate(progress_bar):
                if batch is None:
                    continue
                
                with self.accelerator.accumulate(self.flux_model):
                    loss = self.training_step(batch)
                    self.accelerator.backward(loss)
                    self.optimizer.step()
                    self.lr_scheduler.step()
                    self.optimizer.zero_grad()

                if self.accelerator.sync_gradients:
                    self.global_step += 1
                    if self.global_step % self.cfg_log['log_every'] == 0:
                        lr = self.lr_scheduler.get_last_lr()[0]
                        log_data = {
                            "train/loss": loss.item(),
                            "train/lr": lr,
                            "epoch": epoch,
                        }
                        self.accelerator.log(log_data, step=self.global_step)
                        progress_bar.set_postfix(loss=loss.item())
                    
                    if self.global_step % self.cfg_log['save_every'] == 0:
                        self.save_checkpoint()

        self.accelerator.print("Training complete!")
        self.save_checkpoint(final=True)
        self.accelerator.end_training()

    def save_checkpoint(self, final=False):
        if not self.accelerator.is_main_process:
            return

        step_name = f"step-{self.global_step}"
        if final: step_name = "final"
        save_dir = Path(self.cfg_log['output_dir']) / step_name
        save_dir.mkdir(parents=True, exist_ok=True)
        self.accelerator.print(f"\nğŸ’¾ Saving checkpoint to {save_dir}...")

        unwrapped_flux = self.accelerator.unwrap_model(self.flux_model)
        unwrapped_proj = self.accelerator.unwrap_model(self.text_proj)
        unwrapped_t5_embed = self.accelerator.unwrap_model(self.t5_embed_layer)
        
        flux_lora_state_dict = {
            k: v for k, v in unwrapped_flux.state_dict().items() if "lora" in k
        }
        
        try:
            from safetensors.torch import save_file
            save_file(flux_lora_state_dict, save_dir / "flux_lora.safetensors")
            save_file(unwrapped_proj.state_dict(), save_dir / "text_projection.safetensors")
            save_file(unwrapped_t5_embed.state_dict(), save_dir / "t5_etc_embeddings.safetensors")
        except ImportError:
            torch.save(flux_lora_state_dict, save_dir / "flux_lora.pt")
            torch.save(unwrapped_proj.state_dict(), save_dir / "text_projection.pt")
            torch.save(unwrapped_t5_embed.state_dict(), save_dir / "t5_etc_embeddings.pt")

        with open(save_dir / "config.yaml", 'w') as f:
            yaml.dump(self.config, f)
            
        self.accelerator.print(f"âœ… Checkpoint saved.")

# 4. Script Execution
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to config.yaml file"
    )
    args = parser.parse_args()
    
    # Ensure config path is absolute or relative to execution dir
    # This assumes config.yaml is in the same dir as train.py
    config_path = args.config

    trainer = Trainer(config_path=str(config_path))
    trainer.train()

if __name__ == "__main__":
    main()
=======

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata[idx]
        prompt = item['prompt']
        image_path = os.path.join(self.image_base_path, item['image'])
        
        try:
            image = Image.open(image_path).convert("RGB")
            # ì´ë¯¸ì§€ë¥¼ -1ì—ì„œ 1 ì‚¬ì´ì˜ Torch í…ì„œë¡œ ë³€í™˜ (VAE ì…ë ¥ í˜•ì‹)
            image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() 
            image_tensor = (image_tensor / 127.5) - 1.0 
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜ (collate_fnì´ ì²˜ë¦¬í•¨)
            # print(f"Error loading image {image_path}: {e}")
            return None 

        return image_tensor, prompt


def to_bfloat16_dict(d):
    """ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  í…ì„œë¥¼ bfloat16ìœ¼ë¡œ ë³€í™˜"""
    return {k: v.to(torch.bfloat16) if isinstance(v, torch.Tensor) else v 
            for k, v in d.items()}


def custom_collate_fn(batch):
    """Dataloaderì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨(None) ìƒ˜í”Œì„ í•„í„°ë§"""
    batch = list(filter(lambda x: x is not None, batch))
    if not batch:
        return None 
    
    images = [item[0] for item in batch]
    prompts = [item[1] for item in batch]
    images_batch = torch.stack(images)
    
    return images_batch, prompts

class TrainingPipeline:
    """í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ë¬¶ëŠ” ì»¨í…Œì´ë„ˆ"""
    def __init__(self, t5, clip, ae, model, text_proj=None):
        self.t5 = t5
        self.clip = clip
        self.ae = ae
        self.model = model
        self.text_proj = text_proj

def set_etc_token_trainable(pipeline, is_trainable: bool):
    """mT5 ETC-Tokens ë° Joint Text Attention ê´€ë ¨ ê°€ì¤‘ì¹˜ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •"""
    model = pipeline.model
    t5_module = pipeline.t5.hf_module

    # 1. mT5 Encoder ì „ì²´ ê°€ì¤‘ì¹˜ Freeze
    for name, param in t5_module.named_parameters():
        param.requires_grad_(False)
    
    # 2. ETC-Tokensë§Œ Unfreeze (T5 ì„ë² ë”© ë ˆì´ì–´ì˜ 'shared' íŒŒë¼ë¯¸í„° ê°€ì •)
    # if hasattr(t5_module, 'shared') and isinstance(t5_module.shared, torch.Tensor):
    #     embedding_weight = t5_module.shared
    #     if embedding_weight.shape[0] > ETC_TOKEN_ID_START:
    #         embedding_weight[ETC_TOKEN_ID_START:].requires_grad_(is_trainable)
    #         print(f"mT5 Embedding: {embedding_weight.shape[0] - ETC_TOKEN_ID_START} ETC-Tokens Unfrozen.")

    # 2. ETC-Tokensë§Œ Unfreeze 
    # mT5ì˜ ì‹¤ì œ ì„ë² ë”© ë ˆì´ì–´ ì°¾ê¸°
    if hasattr(t5_module, 'shared'):
        embedding_weight = t5_module.shared
        if isinstance(embedding_weight, torch.nn.Parameter):
            # Parameterì¸ ê²½ìš°
            if embedding_weight.shape[0] > ETC_TOKEN_ID_START:
                embedding_weight.requires_grad_(is_trainable)
                print(f"âœ… mT5: {embedding_weight.shape[0] - ETC_TOKEN_ID_START} ETC-Tokens Unfrozen")
        elif hasattr(embedding_weight, 'weight'):
            # Embedding ë ˆì´ì–´ì¸ ê²½ìš°
            if embedding_weight.weight.shape[0] > ETC_TOKEN_ID_START:
                # ì¼ë¶€ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •í•˜ë ¤ë©´ hook ì‚¬ìš© í•„ìš”
                embedding_weight.weight.requires_grad_(is_trainable)
                print(f"âœ… mT5 Embedding: All tokens unfrozen (including {embedding_weight.weight.shape[0] - ETC_TOKEN_ID_START} ETC)")

    # 3. Text Projection Layer Unfreeze
    if pipeline.text_proj is not None:
        for param in pipeline.text_proj.parameters():
            param.requires_grad_(is_trainable)
        print(f"âœ… Text Projection (768â†’4096): Unfrozen")

    # 4. Flux ëª¨ë¸ì˜ Joint Text Attention (Txt-Attn) ê´€ë ¨ ê°€ì¤‘ì¹˜ Unfreeze
    if hasattr(model, 'txt_in'):
        for param in model.txt_in.parameters():
            param.requires_grad_(is_trainable)
            
    print(f"Txt-in layer requires_grad_({is_trainable})")

# ====================================================================
# 3. ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜
# ====================================================================

# def train_fonts_tc_ft_mt5(model_type=MODEL_TYPE, metadata_path='metadata.jsonl', image_base_path='./'):
#     print(f"Starting FonTS TC Fine-tuning on {DEVICE}...")

#     # --- 1. ëª¨ë¸ ë° í™˜ê²½ ë¡œë“œ ---
#     # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” load_t5, load_clip, load_ae, load_flow_model í˜¸ì¶œ í•„ìš”
#     flux_model = load_flow_model(model_type, DEVICE) # ì›ë³¸ flux ê·¸ëŒ€ë¡œ ë¡œë“œ(4096ì°¨ì›)
#     ae = load_ae(model_type, DEVICE)
    
#     pipeline = TrainingPipeline(load_t5(DEVICE), load_clip(DEVICE), ae, flux_model)
    
#     # Freeze VAE ë° CLIP (Frozen)
#     pipeline.ae.eval().requires_grad_(False)
#     pipeline.clip.hf_module.eval().requires_grad_(False)
    
#     # ETC-Token ë° Txt-Attn í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
#     set_etc_token_trainable(pipeline, is_trainable=True) 

#     # --- 2. ë°ì´í„° ì¤€ë¹„ ---
#     train_dataset = TCDataset(metadata_path=metadata_path, image_base_path=image_base_path)
#     train_loader = DataLoader(
#         train_dataset, 
#         batch_size=BATCH_SIZE, 
#         shuffle=True, 
#         collate_fn=custom_collate_fn,
#         num_workers=4
#     )
    
#     # --- 3. ì˜µí‹°ë§ˆì´ì € ì„¤ì • ---
#     trainable_flux_params = [p for p in pipeline.model.parameters() if p.requires_grad]
#     trainable_t5_params = [p for p in pipeline.t5.hf_module.parameters() if p.requires_grad]
#     trainable_params = trainable_flux_params + trainable_t5_params
    
#     if not trainable_params:
#         raise ValueError("No trainable parameters found! Check set_etc_token_trainable logic.")
    
#     optimizer = optim.AdamW(trainable_params, lr=LEARNING_RATE)
#     pipeline.model.train()
    
#     # --- 4. í›ˆë ¨ ë£¨í”„ ---
#     for epoch in range(NUM_EPOCHS):
#         for step, batch_data in enumerate(train_loader):
#             if batch_data is None: 
#                 continue
                
#             x0_pixel, text_prompt = batch_data
            
#             optimizer.zero_grad()
#             x0_pixel = x0_pixel.to(DEVICE) 
            
#             # (A) VAE ì¸ì½”ë”© (x0_latent)
#             with torch.no_grad():
#                 x0_latent = pipeline.ae.encode(x0_pixel) 
                
#             # (B) ë…¸ì´ì¦ˆ ìƒ˜í”Œë§ ë° zt ê³„ì‚°
#             batch_size = x0_latent.shape[0]
#             t = torch.rand(batch_size, device=DEVICE)
#             epsilon = torch.randn_like(x0_latent, device=DEVICE)

#             # Rectified Flow Matching zt: zt = x0 * (1-t) + epsilon * t
#             zt = x0_latent * (1 - t)[:, None, None, None] + epsilon * t[:, None, None, None]

#             # (C) ì¡°ê±´ë¶€ í…ì„œ ì¤€ë¹„ (mT5 ë° CLIP)
#             # prepare í•¨ìˆ˜ê°€ ztë¥¼ íŒ¨ì¹˜ í˜•íƒœë¡œ ë³€í™˜í•˜ê³  T5/CLIP í˜¸ì¶œí•˜ì—¬ ì¡°ê±´ ìƒì„±
#             inp_cond = prepare(t5=pipeline.t5, clip=pipeline.clip, img=zt, prompt=text_prompt)
            
#             # (D) Flux MM-DiT ìˆœì „íŒŒ: ë…¸ì´ì¦ˆ ì˜ˆì¸¡
#             noise_pred = pipeline.model(
#                 img=inp_cond['img'],
#                 img_ids=inp_cond['img_ids'],
#                 txt=inp_cond['txt'], 
#                 txt_ids=inp_cond['txt_ids'],
#                 timesteps=t,
#                 y=inp_cond['vec'],
#             )
            
#             # (E) ì†ì‹¤ ê³„ì‚° (LCFM = MSE Loss)
#             loss = F.mse_loss(noise_pred, epsilon)
            
#             # (F) ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
#             loss.backward()
#             optimizer.step()

#             if (step + 1) % 50 == 0:
#                 print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Step {step+1}/{len(train_loader)}, Loss: {loss.item():.4f}")

#     print("mT5-based TC Fine-tuning Complete. ğŸ‡°ğŸ‡·")

#     # --- 5. ëª¨ë¸ ì €ì¥ ë¡œì§ ---
#     CHECKPOINT_DIR = "checkpoints"
#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)
#     CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, "fonts_tc_ft_mt5_final.safetensors")

#     state = {
#         'flux_model_state_dict': pipeline.model.state_dict(),
#         't5_embedder_state_dict': pipeline.t5.hf_module.state_dict(),
#         'optimizer_state_dict': optimizer.state_dict(),
#     }

#     # safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
#     # [ì£¼ì˜]: safetensors.torch.save_file í•¨ìˆ˜ëŠ” ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•¨ìˆ˜ì´ë¯€ë¡œ, torch.saveë¡œ ëŒ€ì²´
#     torch.save(state, CHECKPOINT_PATH)
#     print(f"\nâœ… Checkpoint saved to: {CHECKPOINT_PATH}")


def train_fonts_tc_ft_mt5(model_type=MODEL_TYPE, metadata_path='metadata.jsonl', image_base_path='./'):
    print(f"Starting FonTS TC Fine-tuning on {DEVICE}...")

    # --- 1. ëª¨ë¸ ë¡œë“œ (ì›ë³¸ Flux ê·¸ëŒ€ë¡œ ë¡œë“œ) ---
    flux_model = load_flow_model(model_type, DEVICE)  # âš ï¸ 4096 ì°¨ì› ê·¸ëŒ€ë¡œ
    ae = load_ae(model_type, DEVICE)
    t5 = load_t5(DEVICE)
    clip = load_clip(DEVICE)

    # â­ VAEë¥¼ bfloat16ìœ¼ë¡œ ë³€í™˜
    ae = ae.to(torch.bfloat16)
    flux_model = flux_model.to(torch.bfloat16)
    
    # --- 2. Text Projection Layer ì´ˆê¸°í™” â­ ---
    text_proj = TextProjection(input_dim=768, output_dim=4096).to(DEVICE, dtype=torch.bfloat16)
    print("âœ… Text Projection Layer initialized (768 â†’ 4096)")
    
    # --- 3. Pipeline êµ¬ì„± ---
    pipeline = TrainingPipeline(t5, clip, ae, flux_model, text_proj)
    
    # â­ Flux ì „ì²´ë¥¼ ë¨¼ì € Freeze
    pipeline.model.eval()
    for param in pipeline.model.parameters():
        param.requires_grad_(False)
    print("âœ… Flux model fully frozen")

    # Freeze VAE ë° CLIP
    pipeline.ae.eval().requires_grad_(False)
    pipeline.clip.hf_module.eval().requires_grad_(False)
    
    # ETC-Token, Text Projection, Txt-Attn í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
    set_etc_token_trainable(pipeline, is_trainable=True)

    # --- 4. ë°ì´í„° ì¤€ë¹„ ---
    train_dataset = TCDataset(metadata_path=metadata_path, image_base_path=image_base_path)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        collate_fn=custom_collate_fn,
        num_workers=4
    )
    
    # --- 5. ì˜µí‹°ë§ˆì´ì € ì„¤ì • (Text Projection í¬í•¨) â­ ---
    trainable_flux_params = [p for p in pipeline.model.parameters() if p.requires_grad]
    trainable_t5_params = [p for p in pipeline.t5.hf_module.parameters() if p.requires_grad]
    trainable_proj_params = [p for p in pipeline.text_proj.parameters() if p.requires_grad]
    
    trainable_params = trainable_flux_params + trainable_t5_params + trainable_proj_params
    
    if not trainable_params:
        raise ValueError("No trainable parameters found!")
    
    print(f"ğŸ“Š Trainable parameters:")
    print(f"  - Flux (txt_in): {sum(p.numel() for p in trainable_flux_params):,}")
    print(f"  - mT5 (ETC): {sum(p.numel() for p in trainable_t5_params):,}")
    print(f"  - Projection: {sum(p.numel() for p in trainable_proj_params):,}")
    
    optimizer = optim.AdamW(trainable_params, lr=LEARNING_RATE)
    pipeline.model.train()
    pipeline.text_proj.train()  # â­ Projectionë„ train ëª¨ë“œ
    
    # --- 6. í›ˆë ¨ ë£¨í”„ ---
    for epoch in range(NUM_EPOCHS):
        for step, batch_data in enumerate(train_loader):
            if batch_data is None: 
                continue
                
            x0_pixel, text_prompt = batch_data
            optimizer.zero_grad()

            # â­ bfloat16ìœ¼ë¡œ ë³€í™˜
            x0_pixel = x0_pixel.to(DEVICE, dtype=torch.bfloat16)
            
            # (A) VAE ì¸ì½”ë”©
            with torch.no_grad():
                x0_latent = pipeline.ae.encode(x0_pixel)
                
            # (B) ë…¸ì´ì¦ˆ ìƒ˜í”Œë§ ë° zt ê³„ì‚°
            batch_size = x0_latent.shape[0]
            t = torch.rand(batch_size, device=DEVICE, dtype=torch.bfloat16)
            epsilon = torch.randn_like(x0_latent, device=DEVICE, dtype=torch.bfloat16)
            zt = x0_latent * (1 - t)[:, None, None, None] + epsilon * t[:, None, None, None]

            # â­â­ ìƒˆë¡œìš´ guidance í…ì„œ ìƒì„± (Batch Sizeì™€ ë™ì¼í•˜ê²Œ)
            guidance_tensor = torch.full(
                (batch_size,), 
                GUIDANCE_STRENGTH, # ìƒë‹¨ì—ì„œ ì •ì˜í•œ ê°’
                device=DEVICE, 
                dtype=torch.bfloat16 # bfloat16ìœ¼ë¡œ ìƒì„±
            )

            # (C) ì¡°ê±´ë¶€ í…ì„œ ì¤€ë¹„ â­ ìˆ˜ì • í•„ìš”
            inp_cond = prepare(
                t5=pipeline.t5, 
                clip=pipeline.clip, 
                img=zt, 
                prompt=text_prompt,
                dtype=torch.bfloat16 
            )
            
            inp_cond = to_bfloat16_dict(inp_cond)

            # ë””ë²„ê¹… ì¶œë ¥ ì¶”ê°€
            print(f"DEBUG: inp_cond['txt'] shape before projection: {inp_cond['txt'].shape}")

            # â­ mT5 ì„ë² ë”©ì„ Projection Layerë¡œ ë³€í™˜
            # inp_cond['txt']ëŠ” (B, L, 768) í˜•íƒœ
            # Text Projection (bfloat16 ìœ ì§€)
            txt_projected = pipeline.text_proj(inp_cond['txt'].to(torch.bfloat16))  # (B, L, 4096)
            
            # ë””ë²„ê¹… ì¶œë ¥ ì¶”ê°€
            print(f"DEBUG: txt_projected shape after projection: {txt_projected.shape}") 
            print(f"DEBUG: txt_ids shape: {inp_cond['txt_ids'].shape}") 
            print(f"DEBUG: img shape: {inp_cond['img'].shape}") 
            print(f"DEBUG: img_ids shape: {inp_cond['img_ids'].shape}")


            # (D) Flux MM-DiT ìˆœì „íŒŒ
            noise_pred = pipeline.model(
                img=inp_cond['img'],
                img_ids=inp_cond['img_ids'],
                txt=txt_projected,  # â­ ë³€í™˜ëœ í…ì„œ ì‚¬ìš©
                txt_ids=inp_cond['txt_ids'],
                timesteps=t,
                y=inp_cond['vec'],
                guidance=guidance_tensor,
            )
            
            # (E) ì†ì‹¤ ê³„ì‚°
            loss = F.mse_loss(noise_pred, epsilon)
            
            # (F) ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸
            loss.backward()
            optimizer.step()

            if (step + 1) % 50 == 0:
                print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Step {step+1}, Loss: {loss.item():.4f}")

    print("âœ… mT5-based TC Fine-tuning Complete!")

    # --- 7. ëª¨ë¸ ì €ì¥ (Text Projection í¬í•¨) â­ ---
    CHECKPOINT_DIR = "checkpoints"
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, "fonts_tc_ft_mt5_final.safetensors")

    state = {
        'flux_model_state_dict': pipeline.model.state_dict(),
        't5_embedder_state_dict': pipeline.t5.hf_module.state_dict(),
        'text_projection_state_dict': pipeline.text_proj.state_dict(),  # â­ ì¶”ê°€
        'optimizer_state_dict': optimizer.state_dict(),
    }

    torch.save(state, CHECKPOINT_PATH)
    print(f"âœ… Checkpoint saved to: {CHECKPOINT_PATH}")


if __name__ == "__main__":
    # ì‚¬ìš©ìë‹˜ì˜ í™˜ê²½ì— ë§ê²Œ metadata íŒŒì¼ ê²½ë¡œì™€ ì´ë¯¸ì§€ ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ:
    METADATA_PATH = "/data1/FonTS/flux+SCA-both/src/flux/tc-dataset/metadata.jsonl"
    IMAGE_BASE_PATH = "/data1/FonTS/flux+SCA-both/src/flux" 
    
    try:
        train_fonts_tc_ft_mt5(
            model_type="flux-dev", 
            metadata_path=METADATA_PATH, 
            image_base_path=IMAGE_BASE_PATH
        )
    except ValueError as e:
        print(f"Error during training setup: {e}")
        print("Please ensure the Flux model parameters (context_in_dim, etc.) are correctly configured in external files (like utils.py) to match the mT5 embedding dimension (768).")
>>>>>>> theirs/main

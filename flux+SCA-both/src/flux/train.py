# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
from transformers import AutoTokenizer # ETC-Token ì²˜ë¦¬ë¥¼ ìœ„í•´ í† í¬ë‚˜ì´ì € ì‚¬ìš© ê°€ì •
import math
import os
import numpy as np
import json
from PIL import Image
import random

from .sampling import prepare
from .util import load_flow_model, load_ae

os.environ['AE'] = '/data1/FonTS/flux+SCA-both/src/models/ae.safetensors'
os.environ['FLUX_DEV'] = '/data1/FonTS/flux+SCA-both/src/models/flux1-dev.safetensors'


"""
 ì‹¤ì œ $\text{Flux}$ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜(load_ae, load_flow_model, load_t5, load_clip) ë° 
 $\text{Flux}$ í´ë˜ìŠ¤ ì •ì˜ê°€ ì™¸ë¶€ íŒŒì¼(utils.py, model.py ë“±)ì— ì •í™•íˆ ì¡´ì¬í•˜ê³  í˜¸ì¶œ ê°€ëŠ¥í•¨ì„ ê°€ì •
"""

# ====================================================================
# 1. íŒŒë¼ë¯¸í„° ë° ë”ë¯¸ í´ë˜ìŠ¤ ì •ì˜ (ì™¸ë¶€ ëª¨ë“ˆì„ ëª¨ë°©)
# ====================================================================

# íŒŒë¼ë¯¸í„° ì •ì˜ (í•˜ë“œì›¨ì–´ ë° í›ˆë ¨ íš¨ìœ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ BATCH_SIZE=1 ì„¤ì • ìœ ì§€)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_TYPE = "flux-dev"
LEARNING_RATE = 1e-5
BATCH_SIZE = 1 # ë©”ëª¨ë¦¬ ì œì•½ì„ ê³ ë ¤í•˜ì—¬ 1ë¡œ ì„¤ì •
NUM_EPOCHS = 10
# â­ ì¶”ê°€: Guidance Distillation Strength ì„¤ì •
GUIDANCE_STRENGTH = 7.0 # ì¼ë°˜ì ì¸ ê°’ (4.0 ~ 10.0 ì‚¬ì´ì—ì„œ í…ŒìŠ¤íŠ¸ í•„ìš”)

# --- T5/mT5 ë¡œë”© í•¨ìˆ˜ ì •ì˜ (ì‹¤ì œ ëª¨ë“ˆì„ ëª¨ë°©í•œ Dummy í´ë˜ìŠ¤) ---
MT5_MODEL_NAME = "google/mt5-base" # mT5 ì‚¬ìš©ì„ ìœ„í•´ ë¡œë”© ì´ë¦„ ì •ì˜
ETC_TOKEN_ID_START = 50000 

class TextProjection(nn.Module):
    """mT5 ì„ë² ë”©(768ì°¨ì›)ì„ T5-XXL í˜¸í™˜(4096ì°¨ì›)ìœ¼ë¡œ ë³€í™˜"""
    def __init__(self, input_dim=768, output_dim=4096):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.LayerNorm(output_dim),  # ì•ˆì •ì„± í–¥ìƒ
        )
    
    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, 768) - mT5 ì„ë² ë”©
        Returns:
            (batch, seq_len, 4096) - T5-XXL í˜¸í™˜ ì„ë² ë”©
        """
        return self.proj(x)

class DummyHFEmbedder:
    """ì‹¤ì œ mT5/CLIP ì¸ì½”ë” ëª¨ë“ˆì„ ëŒ€ì²´"""
    def __init__(self, version, max_length, torch_dtype):
        self.output_dim = 768  # mT5/CLIP-Baseì˜ ì¼ë°˜ì ì¸ ì„ë² ë”© ì°¨ì› ê°€ì •
        # ì‹¤ì œ T5/CLIP ëª¨ë“ˆì„ ë¡œë“œí–ˆë‹¤ê³  ê°€ì •í•˜ê³ , ì„ë² ë”©ì„ ìœ„í•œ ë”ë¯¸ ë ˆì´ì–´ë¥¼ ì •ì˜
        self.hf_module = torch.nn.Linear(768, 768) 
        # mT5ì˜ ì„ë² ë”© ë ˆì´ì–´ ì´ë¦„ì€ 'shared'ì¼ ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„œëŠ” ë”ë¯¸ë¡œ ì •ì˜
        self.hf_module.shared = torch.nn.Parameter(torch.randn(50000 + 100, 768)) 
        
    def to(self, device): return self
    def eval(self): return self
    def requires_grad_(self, requires): return self
    # __call__ì€ prepare í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë¨. (B, L, D) í…ì„œ ë°˜í™˜ ê°€ì •
    def __call__(self, text): 
        return torch.randn(len(text), 50, self.output_dim, device=DEVICE) 
    
def load_clip(device) -> DummyHFEmbedder: 
    return DummyHFEmbedder("CLIP", 77, torch.bfloat16).to(device)

def load_t5(device: str | torch.device = "cuda", max_length: int = 512) -> DummyHFEmbedder: 
    print(f"Loading mT5 model: {MT5_MODEL_NAME}")
    return DummyHFEmbedder(MT5_MODEL_NAME, max_length=max_length, torch_dtype=torch.bfloat16).to(device)

class DummyAE:
    """ì‹¤ì œ VAE ì¸ì½”ë”/ë””ì½”ë”ë¥¼ ëŒ€ì²´"""
    def encode(self, x): 
        # VAEëŠ” (B, 3, H, W) -> (B, 64, H/16, W/16) (ì˜ˆì‹œ)ë¡œ ë³€í™˜
        return torch.randn(x.shape[0], 64, 32, 32, device=DEVICE) 
    def eval(self): return self
    def requires_grad_(self, requires): return self

class DummyFlux(torch.nn.Module):
    """ì‹¤ì œ Flux(MM-DiT) ë°±ë³¸ì„ ëŒ€ì²´"""
    def __init__(self):
        super().__init__()
        # ë…¼ë¬¸ì˜ txt_in ë ˆì´ì–´ë¥¼ ëª¨ë°©
        self.txt_in = nn.Linear(768, 3072) 
    def forward(self, img, img_ids, txt, txt_ids, timesteps, y, **kwargs):
        return torch.randn(img.shape[0], 64, 32, 32, device=DEVICE) 
    # ì‹¤ì œ Flux ëª¨ë¸ì´ ê°€ì§€ê³  ìˆì–´ì•¼ í•  set_etc_token_trainableì´ ì°¸ì¡°í•˜ëŠ” ì†ì„±
    # txt_inì´ nn.Moduleì´ë¯€ë¡œ parameters()ë¥¼ í†µí•´ ì ‘ê·¼ ê°€ëŠ¥í•¨

# ====================================================================
# 2. Dataset ë° íŒŒë¼ë¯¸í„° ì„¤ì • í•¨ìˆ˜
# ====================================================================

class TCDataset(Dataset):
    """metadata.jsonl ë° ì´ë¯¸ì§€ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì‹¤ì œ ë°ì´í„°ì…‹ ë¡œë”"""
    def __init__(self, metadata_path: str, image_base_path: str = './'):
        self.metadata = []
        with open(metadata_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.metadata.append(json.loads(line))
        self.image_base_path = image_base_path

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata[idx]
        prompt = item['prompt']
        image_path = os.path.join(self.image_base_path, item['image'])
        
        try:
            image = Image.open(image_path).convert("RGB")
            # ì´ë¯¸ì§€ë¥¼ -1ì—ì„œ 1 ì‚¬ì´ì˜ Torch í…ì„œë¡œ ë³€í™˜ (VAE ì…ë ¥ í˜•ì‹)
            image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() 
            image_tensor = (image_tensor / 127.5) - 1.0 
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜ (collate_fnì´ ì²˜ë¦¬í•¨)
            # print(f"Error loading image {image_path}: {e}")
            return None 

        return image_tensor, prompt


def to_bfloat16_dict(d):
    """ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  í…ì„œë¥¼ bfloat16ìœ¼ë¡œ ë³€í™˜"""
    return {k: v.to(torch.bfloat16) if isinstance(v, torch.Tensor) else v 
            for k, v in d.items()}


def custom_collate_fn(batch):
    """Dataloaderì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨(None) ìƒ˜í”Œì„ í•„í„°ë§"""
    batch = list(filter(lambda x: x is not None, batch))
    if not batch:
        return None 
    
    images = [item[0] for item in batch]
    prompts = [item[1] for item in batch]
    images_batch = torch.stack(images)
    
    return images_batch, prompts

class TrainingPipeline:
    """í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ë¬¶ëŠ” ì»¨í…Œì´ë„ˆ"""
    def __init__(self, t5, clip, ae, model, text_proj=None):
        self.t5 = t5
        self.clip = clip
        self.ae = ae
        self.model = model
        self.text_proj = text_proj

def set_etc_token_trainable(pipeline, is_trainable: bool):
    """mT5 ETC-Tokens ë° Joint Text Attention ê´€ë ¨ ê°€ì¤‘ì¹˜ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •"""
    model = pipeline.model
    t5_module = pipeline.t5.hf_module

    # 1. mT5 Encoder ì „ì²´ ê°€ì¤‘ì¹˜ Freeze
    for name, param in t5_module.named_parameters():
        param.requires_grad_(False)
    
    # 2. ETC-Tokensë§Œ Unfreeze (T5 ì„ë² ë”© ë ˆì´ì–´ì˜ 'shared' íŒŒë¼ë¯¸í„° ê°€ì •)
    # if hasattr(t5_module, 'shared') and isinstance(t5_module.shared, torch.Tensor):
    #     embedding_weight = t5_module.shared
    #     if embedding_weight.shape[0] > ETC_TOKEN_ID_START:
    #         embedding_weight[ETC_TOKEN_ID_START:].requires_grad_(is_trainable)
    #         print(f"mT5 Embedding: {embedding_weight.shape[0] - ETC_TOKEN_ID_START} ETC-Tokens Unfrozen.")

    # 2. ETC-Tokensë§Œ Unfreeze 
    # mT5ì˜ ì‹¤ì œ ì„ë² ë”© ë ˆì´ì–´ ì°¾ê¸°
    if hasattr(t5_module, 'shared'):
        embedding_weight = t5_module.shared
        if isinstance(embedding_weight, torch.nn.Parameter):
            # Parameterì¸ ê²½ìš°
            if embedding_weight.shape[0] > ETC_TOKEN_ID_START:
                embedding_weight.requires_grad_(is_trainable)
                print(f"âœ… mT5: {embedding_weight.shape[0] - ETC_TOKEN_ID_START} ETC-Tokens Unfrozen")
        elif hasattr(embedding_weight, 'weight'):
            # Embedding ë ˆì´ì–´ì¸ ê²½ìš°
            if embedding_weight.weight.shape[0] > ETC_TOKEN_ID_START:
                # ì¼ë¶€ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •í•˜ë ¤ë©´ hook ì‚¬ìš© í•„ìš”
                embedding_weight.weight.requires_grad_(is_trainable)
                print(f"âœ… mT5 Embedding: All tokens unfrozen (including {embedding_weight.weight.shape[0] - ETC_TOKEN_ID_START} ETC)")

    # 3. Text Projection Layer Unfreeze
    if pipeline.text_proj is not None:
        for param in pipeline.text_proj.parameters():
            param.requires_grad_(is_trainable)
        print(f"âœ… Text Projection (768â†’4096): Unfrozen")

    # 4. Flux ëª¨ë¸ì˜ Joint Text Attention (Txt-Attn) ê´€ë ¨ ê°€ì¤‘ì¹˜ Unfreeze
    if hasattr(model, 'txt_in'):
        for param in model.txt_in.parameters():
            param.requires_grad_(is_trainable)
            
    print(f"Txt-in layer requires_grad_({is_trainable})")

# ====================================================================
# 3. ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜
# ====================================================================

# def train_fonts_tc_ft_mt5(model_type=MODEL_TYPE, metadata_path='metadata.jsonl', image_base_path='./'):
#     print(f"Starting FonTS TC Fine-tuning on {DEVICE}...")

#     # --- 1. ëª¨ë¸ ë° í™˜ê²½ ë¡œë“œ ---
#     # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” load_t5, load_clip, load_ae, load_flow_model í˜¸ì¶œ í•„ìš”
#     flux_model = load_flow_model(model_type, DEVICE) # ì›ë³¸ flux ê·¸ëŒ€ë¡œ ë¡œë“œ(4096ì°¨ì›)
#     ae = load_ae(model_type, DEVICE)
    
#     pipeline = TrainingPipeline(load_t5(DEVICE), load_clip(DEVICE), ae, flux_model)
    
#     # Freeze VAE ë° CLIP (Frozen)
#     pipeline.ae.eval().requires_grad_(False)
#     pipeline.clip.hf_module.eval().requires_grad_(False)
    
#     # ETC-Token ë° Txt-Attn í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
#     set_etc_token_trainable(pipeline, is_trainable=True) 

#     # --- 2. ë°ì´í„° ì¤€ë¹„ ---
#     train_dataset = TCDataset(metadata_path=metadata_path, image_base_path=image_base_path)
#     train_loader = DataLoader(
#         train_dataset, 
#         batch_size=BATCH_SIZE, 
#         shuffle=True, 
#         collate_fn=custom_collate_fn,
#         num_workers=4
#     )
    
#     # --- 3. ì˜µí‹°ë§ˆì´ì € ì„¤ì • ---
#     trainable_flux_params = [p for p in pipeline.model.parameters() if p.requires_grad]
#     trainable_t5_params = [p for p in pipeline.t5.hf_module.parameters() if p.requires_grad]
#     trainable_params = trainable_flux_params + trainable_t5_params
    
#     if not trainable_params:
#         raise ValueError("No trainable parameters found! Check set_etc_token_trainable logic.")
    
#     optimizer = optim.AdamW(trainable_params, lr=LEARNING_RATE)
#     pipeline.model.train()
    
#     # --- 4. í›ˆë ¨ ë£¨í”„ ---
#     for epoch in range(NUM_EPOCHS):
#         for step, batch_data in enumerate(train_loader):
#             if batch_data is None: 
#                 continue
                
#             x0_pixel, text_prompt = batch_data
            
#             optimizer.zero_grad()
#             x0_pixel = x0_pixel.to(DEVICE) 
            
#             # (A) VAE ì¸ì½”ë”© (x0_latent)
#             with torch.no_grad():
#                 x0_latent = pipeline.ae.encode(x0_pixel) 
                
#             # (B) ë…¸ì´ì¦ˆ ìƒ˜í”Œë§ ë° zt ê³„ì‚°
#             batch_size = x0_latent.shape[0]
#             t = torch.rand(batch_size, device=DEVICE)
#             epsilon = torch.randn_like(x0_latent, device=DEVICE)

#             # Rectified Flow Matching zt: zt = x0 * (1-t) + epsilon * t
#             zt = x0_latent * (1 - t)[:, None, None, None] + epsilon * t[:, None, None, None]

#             # (C) ì¡°ê±´ë¶€ í…ì„œ ì¤€ë¹„ (mT5 ë° CLIP)
#             # prepare í•¨ìˆ˜ê°€ ztë¥¼ íŒ¨ì¹˜ í˜•íƒœë¡œ ë³€í™˜í•˜ê³  T5/CLIP í˜¸ì¶œí•˜ì—¬ ì¡°ê±´ ìƒì„±
#             inp_cond = prepare(t5=pipeline.t5, clip=pipeline.clip, img=zt, prompt=text_prompt)
            
#             # (D) Flux MM-DiT ìˆœì „íŒŒ: ë…¸ì´ì¦ˆ ì˜ˆì¸¡
#             noise_pred = pipeline.model(
#                 img=inp_cond['img'],
#                 img_ids=inp_cond['img_ids'],
#                 txt=inp_cond['txt'], 
#                 txt_ids=inp_cond['txt_ids'],
#                 timesteps=t,
#                 y=inp_cond['vec'],
#             )
            
#             # (E) ì†ì‹¤ ê³„ì‚° (LCFM = MSE Loss)
#             loss = F.mse_loss(noise_pred, epsilon)
            
#             # (F) ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
#             loss.backward()
#             optimizer.step()

#             if (step + 1) % 50 == 0:
#                 print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Step {step+1}/{len(train_loader)}, Loss: {loss.item():.4f}")

#     print("mT5-based TC Fine-tuning Complete. ğŸ‡°ğŸ‡·")

#     # --- 5. ëª¨ë¸ ì €ì¥ ë¡œì§ ---
#     CHECKPOINT_DIR = "checkpoints"
#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)
#     CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, "fonts_tc_ft_mt5_final.safetensors")

#     state = {
#         'flux_model_state_dict': pipeline.model.state_dict(),
#         't5_embedder_state_dict': pipeline.t5.hf_module.state_dict(),
#         'optimizer_state_dict': optimizer.state_dict(),
#     }

#     # safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
#     # [ì£¼ì˜]: safetensors.torch.save_file í•¨ìˆ˜ëŠ” ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•¨ìˆ˜ì´ë¯€ë¡œ, torch.saveë¡œ ëŒ€ì²´
#     torch.save(state, CHECKPOINT_PATH)
#     print(f"\nâœ… Checkpoint saved to: {CHECKPOINT_PATH}")


def train_fonts_tc_ft_mt5(model_type=MODEL_TYPE, metadata_path='metadata.jsonl', image_base_path='./'):
    print(f"Starting FonTS TC Fine-tuning on {DEVICE}...")

    # --- 1. ëª¨ë¸ ë¡œë“œ (ì›ë³¸ Flux ê·¸ëŒ€ë¡œ ë¡œë“œ) ---
    flux_model = load_flow_model(model_type, DEVICE)  # âš ï¸ 4096 ì°¨ì› ê·¸ëŒ€ë¡œ
    ae = load_ae(model_type, DEVICE)
    t5 = load_t5(DEVICE)
    clip = load_clip(DEVICE)

    # â­ VAEë¥¼ bfloat16ìœ¼ë¡œ ë³€í™˜
    ae = ae.to(torch.bfloat16)
    flux_model = flux_model.to(torch.bfloat16)
    
    # --- 2. Text Projection Layer ì´ˆê¸°í™” â­ ---
    text_proj = TextProjection(input_dim=768, output_dim=4096).to(DEVICE, dtype=torch.bfloat16)
    print("âœ… Text Projection Layer initialized (768 â†’ 4096)")
    
    # --- 3. Pipeline êµ¬ì„± ---
    pipeline = TrainingPipeline(t5, clip, ae, flux_model, text_proj)
    
    # â­ Flux ì „ì²´ë¥¼ ë¨¼ì € Freeze
    pipeline.model.eval()
    for param in pipeline.model.parameters():
        param.requires_grad_(False)
    print("âœ… Flux model fully frozen")

    # Freeze VAE ë° CLIP
    pipeline.ae.eval().requires_grad_(False)
    pipeline.clip.hf_module.eval().requires_grad_(False)
    
    # ETC-Token, Text Projection, Txt-Attn í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
    set_etc_token_trainable(pipeline, is_trainable=True)

    # --- 4. ë°ì´í„° ì¤€ë¹„ ---
    train_dataset = TCDataset(metadata_path=metadata_path, image_base_path=image_base_path)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        collate_fn=custom_collate_fn,
        num_workers=4
    )
    
    # --- 5. ì˜µí‹°ë§ˆì´ì € ì„¤ì • (Text Projection í¬í•¨) â­ ---
    trainable_flux_params = [p for p in pipeline.model.parameters() if p.requires_grad]
    trainable_t5_params = [p for p in pipeline.t5.hf_module.parameters() if p.requires_grad]
    trainable_proj_params = [p for p in pipeline.text_proj.parameters() if p.requires_grad]
    
    trainable_params = trainable_flux_params + trainable_t5_params + trainable_proj_params
    
    if not trainable_params:
        raise ValueError("No trainable parameters found!")
    
    print(f"ğŸ“Š Trainable parameters:")
    print(f"  - Flux (txt_in): {sum(p.numel() for p in trainable_flux_params):,}")
    print(f"  - mT5 (ETC): {sum(p.numel() for p in trainable_t5_params):,}")
    print(f"  - Projection: {sum(p.numel() for p in trainable_proj_params):,}")
    
    optimizer = optim.AdamW(trainable_params, lr=LEARNING_RATE)
    pipeline.model.train()
    pipeline.text_proj.train()  # â­ Projectionë„ train ëª¨ë“œ
    
    # --- 6. í›ˆë ¨ ë£¨í”„ ---
    for epoch in range(NUM_EPOCHS):
        for step, batch_data in enumerate(train_loader):
            if batch_data is None: 
                continue
                
            x0_pixel, text_prompt = batch_data
            optimizer.zero_grad()

            # â­ bfloat16ìœ¼ë¡œ ë³€í™˜
            x0_pixel = x0_pixel.to(DEVICE, dtype=torch.bfloat16)
            
            # (A) VAE ì¸ì½”ë”©
            with torch.no_grad():
                x0_latent = pipeline.ae.encode(x0_pixel)
                
            # (B) ë…¸ì´ì¦ˆ ìƒ˜í”Œë§ ë° zt ê³„ì‚°
            batch_size = x0_latent.shape[0]
            t = torch.rand(batch_size, device=DEVICE, dtype=torch.bfloat16)
            epsilon = torch.randn_like(x0_latent, device=DEVICE, dtype=torch.bfloat16)
            zt = x0_latent * (1 - t)[:, None, None, None] + epsilon * t[:, None, None, None]

            # â­â­ ìƒˆë¡œìš´ guidance í…ì„œ ìƒì„± (Batch Sizeì™€ ë™ì¼í•˜ê²Œ)
            guidance_tensor = torch.full(
                (batch_size,), 
                GUIDANCE_STRENGTH, # ìƒë‹¨ì—ì„œ ì •ì˜í•œ ê°’
                device=DEVICE, 
                dtype=torch.bfloat16 # bfloat16ìœ¼ë¡œ ìƒì„±
            )

            # (C) ì¡°ê±´ë¶€ í…ì„œ ì¤€ë¹„ â­ ìˆ˜ì • í•„ìš”
            inp_cond = prepare(
                t5=pipeline.t5, 
                clip=pipeline.clip, 
                img=zt, 
                prompt=text_prompt,
                dtype=torch.bfloat16 
            )
            
            inp_cond = to_bfloat16_dict(inp_cond)

            # ë””ë²„ê¹… ì¶œë ¥ ì¶”ê°€
            print(f"DEBUG: inp_cond['txt'] shape before projection: {inp_cond['txt'].shape}")

            # â­ mT5 ì„ë² ë”©ì„ Projection Layerë¡œ ë³€í™˜
            # inp_cond['txt']ëŠ” (B, L, 768) í˜•íƒœ
            # Text Projection (bfloat16 ìœ ì§€)
            txt_projected = pipeline.text_proj(inp_cond['txt'].to(torch.bfloat16))  # (B, L, 4096)
            
            # ë””ë²„ê¹… ì¶œë ¥ ì¶”ê°€
            print(f"DEBUG: txt_projected shape after projection: {txt_projected.shape}") 
            print(f"DEBUG: txt_ids shape: {inp_cond['txt_ids'].shape}") 
            print(f"DEBUG: img shape: {inp_cond['img'].shape}") 
            print(f"DEBUG: img_ids shape: {inp_cond['img_ids'].shape}")


            # (D) Flux MM-DiT ìˆœì „íŒŒ
            noise_pred = pipeline.model(
                img=inp_cond['img'],
                img_ids=inp_cond['img_ids'],
                txt=txt_projected,  # â­ ë³€í™˜ëœ í…ì„œ ì‚¬ìš©
                txt_ids=inp_cond['txt_ids'],
                timesteps=t,
                y=inp_cond['vec'],
                guidance=guidance_tensor,
            )
            
            # (E) ì†ì‹¤ ê³„ì‚°
            loss = F.mse_loss(noise_pred, epsilon)
            
            # (F) ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸
            loss.backward()
            optimizer.step()

            if (step + 1) % 50 == 0:
                print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Step {step+1}, Loss: {loss.item():.4f}")

    print("âœ… mT5-based TC Fine-tuning Complete!")

    # --- 7. ëª¨ë¸ ì €ì¥ (Text Projection í¬í•¨) â­ ---
    CHECKPOINT_DIR = "checkpoints"
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, "fonts_tc_ft_mt5_final.safetensors")

    state = {
        'flux_model_state_dict': pipeline.model.state_dict(),
        't5_embedder_state_dict': pipeline.t5.hf_module.state_dict(),
        'text_projection_state_dict': pipeline.text_proj.state_dict(),  # â­ ì¶”ê°€
        'optimizer_state_dict': optimizer.state_dict(),
    }

    torch.save(state, CHECKPOINT_PATH)
    print(f"âœ… Checkpoint saved to: {CHECKPOINT_PATH}")


if __name__ == "__main__":
    # ì‚¬ìš©ìë‹˜ì˜ í™˜ê²½ì— ë§ê²Œ metadata íŒŒì¼ ê²½ë¡œì™€ ì´ë¯¸ì§€ ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ:
    METADATA_PATH = "/data1/FonTS/flux+SCA-both/src/flux/tc-dataset/metadata.jsonl"
    IMAGE_BASE_PATH = "/data1/FonTS/flux+SCA-both/src/flux" 
    
    try:
        train_fonts_tc_ft_mt5(
            model_type="flux-dev", 
            metadata_path=METADATA_PATH, 
            image_base_path=IMAGE_BASE_PATH
        )
    except ValueError as e:
        print(f"Error during training setup: {e}")
        print("Please ensure the Flux model parameters (context_in_dim, etc.) are correctly configured in external files (like utils.py) to match the mT5 embedding dimension (768).")